{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaouad/sharkfest-us-2024/blob/test-colab/notebooks/wifi-health-checks-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DM8COOjidJK"
      },
      "source": [
        "#  Enhancing Wi-Fi Networks with AI: A Deep Dive into Machine Learning for Wi-Fi Health Checks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTY-6KrIcTFd"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "\n",
        "In today's connected world, reliable Wi-Fi is crucial. But keeping it running smoothly can be tricky with all the potential issues like interference and signal drops. We're here to show you how AI, specifically supervised machine learning, can help spot and fix decrease of data ratefor a moving station. We'll walk you through how to build a model that detects performance drops in Wi-Fi networks, making network management more proactive and efficient.\n",
        "\n",
        "Whether you're a tech expert or just curious about AI, this workshop will give you practical insights and hands-on experience. Let's dive in and explore how machine learning can make Wi-Fi better for everyone!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai_CGEl3opN3"
      },
      "source": [
        "### Objectives\n",
        "* Get Familiar with Supervised Learning: Learn the basics of supervised learning, including key concepts, algorithms, and how it's used to make predictions based on labeled data.\n",
        "\n",
        "* Introduction to Time Series Modeling for Machine Learning: Gain a gentle introduction to time series modeling, exploring how to handle sequential data and apply it in a machine learning context.\n",
        "\n",
        "* Apply Time Series Modeling to Detect Decreases in Data Rate: Discover practical applications by using time series modeling to identify and address drops in Wi-Fi performance, specifically focusing on detecting decreases in data rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhmN91DYcTFd"
      },
      "source": [
        "### Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbLUpGWNcTFe"
      },
      "source": [
        "**TODO**: Murat to add the details of how data was generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSa2j5e7cTFe"
      },
      "source": [
        "### PCAP Extraction process\n",
        "To extract the content of the PCAP file we used the following command:\n",
        "\n",
        "```\n",
        "tshark -o 'gui.column.format:\"cus.protocol\",\"%p\",\"Protocol\",\"%p\",\"cus.wlan.fc.type\",\"%Cus:wlan.fc.type:0:R\",\"cus.wlan.fc.type_subtype\",\"%Cus:wlan.fc.type_subtype:0:R\",\"cus.wlan.ssid\",\"%Cus:wlan.ssid:0:R\"' \\\n",
        "-T fields \\\n",
        "-e _ws.col.cus.protocol \\\n",
        "-e _ws.col.Protocol \\\n",
        "-e frame.protocols \\\n",
        "-e frame.number \\\n",
        "-e frame.time_epoch \\\n",
        "-e wlan.fc.type \\\n",
        "-e _ws.col.cus.wlan.fc.type \\\n",
        "-e wlan.fc.type_subtype \\\n",
        "-e _ws.col.cus.wlan.fc.type_subtype \\\n",
        "-e wlan.ssid \\\n",
        "-e _ws.col.cus.wlan.ssid \\\n",
        "-e wlan.tag \\\n",
        "-e wlan.bssid \\\n",
        "-e wlan.sa \\\n",
        "-e wlan.da \\\n",
        "-e wlan.ra \\\n",
        "-e wlan.ta \\\n",
        "-e wlan_radio.channel \\\n",
        "-e wlan_radio.signal_dbm \\\n",
        "-e wlan_radio.data_rate \\\n",
        "-e wlan.rsn.capabilities \\\n",
        "-e wlan.rsn.akms.list \\\n",
        "-e wlan.rsn.pcs.list \\\n",
        "-e wlan.qos.tid \\\n",
        "-e radiotap.channel.freq \\\n",
        "-e radiotap.dbm_antsignal \\\n",
        "-e radiotap.datarate \\\n",
        "-e wlan.addr \\\n",
        "-e wlan.seq \\\n",
        "-e wlan.fc.retry \\\n",
        "-e wlan.fc.retry.expert \\\n",
        "-e radiotap.data_retries \\\n",
        "-e wlan.duration \\\n",
        "-e wlan.qbss.cu \\\n",
        "-e radiotap.quality \\\n",
        "-e wlan_radio.signal_percentage \\\n",
        "-e wlan.fcs \\\n",
        "-e wlan.fcs.status \\\n",
        "-e wlan.fc \\\n",
        "-e wlan.fc.moredata \\\n",
        "-e frame.len \\\n",
        "-e wlan_radio.duration \\\n",
        "-e frame.cap_len \\\n",
        "-e wlan.qos.priority \\\n",
        "-e wlan.qos.ack \\\n",
        "-e wlan.qos.buf_state_indicated \\\n",
        "-e wlan_radio.noise_dbm \\\n",
        "-e tcp.analysis.ack_rtt \\\n",
        "-e tcp.analysis.lost_segment \\\n",
        "-e tcp.analysis.retransmission \\\n",
        "-e frame.time_delta \\\n",
        "-e wlan_radio.snr \\\n",
        "-e wlan.fc.more_fragments \\\n",
        "-e wlan.fixed.ssc.fragment \\\n",
        "-e wlan.fragment \\\n",
        "-e wlan.fragment.count \\\n",
        "-e wlan.fragment.error \\\n",
        "-e wlan_radio.frequency \\\n",
        "-e radiotap.present.db_antnoise \\\n",
        "-e radiotap.present.db_antsignal \\\n",
        "-e radiotap.present.dbm_antnoise \\\n",
        "-e radiotap.present.dbm_antsignal \\\n",
        "-E aggregator=\"$\" \\\n",
        "-E separator=/t \\\n",
        "-E header=y \\\n",
        "-r 'wifi_file.pcap' \\\n",
        "-w 'output_file.csv'\n",
        "```\n",
        "\n",
        "\n",
        "where `wifi_file.pcap` is the PCAP file and `output.csv` is csv structured output that we will use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkE7wl73pUhe"
      },
      "source": [
        "## Pre-processing of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsgAeb7pcTFe",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import output\n",
        "    # output.enable_custom_widget_manager()         # Enable custom widgets in Colab for profile reports\n",
        "    IN_COLAB = True\n",
        "    # Load the autoreload extension for IPython\n",
        "    %load_ext autoreload\n",
        "    # Set the autoreload extension to reload modules every time they are imported, so that changes made to code in the src folder are reflected in the running code\n",
        "    %autoreload 2\n",
        "\n",
        "    %pip install ydata_profiling==4.8.3 \\\n",
        "            scikit-learn==1.5.0 \\\n",
        "            itables==2.1.0 \\\n",
        "            ipywidgets==8.1.2 \\\n",
        "            numpy==1.24.1\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET-H7HyPpFWd",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Import relevant python libraries\n",
        "\n",
        "import os                       # Interact with the operating system\n",
        "import pandas as pd             # Data analysis and manipulation\n",
        "import numpy as np              # Numerical operations on arrays\n",
        "import matplotlib.pyplot as plt         # Create plots\n",
        "from itables import show                # To create interactive tables\n",
        "from ydata_profiling import ProfileReport     # Generate data profile reports\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iITmyJ0vcTFg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Loading data from remote repository\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from ipywidgets import interact\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()         # Enable custom widgets in Colab for profile reports\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "  # Define the URL of the zip file\n",
        "  url = \"https://github.com/b-yond-infinite-network/sharkfest-us-2024/raw/main/data/wifi-health-checks-data.zip\"\n",
        "\n",
        "  # Define the local file path where the zip file will be saved\n",
        "  data_path = \"/content/wifi-health-checks-data.zip\"\n",
        "  local_extract_path =\"/content/tmp\"\n",
        "\n",
        "  # Download the zip file using wget\n",
        "  !wget -O {data_path} {url}\n",
        "\n",
        "  # Confirm the download location\n",
        "  print(f\"Zip file downloaded to: {local_extract_path}\")\n",
        "else:\n",
        "  # Define the path of the zip file\n",
        "  data_path = \"../data/wifi-health-checks-data.zip\"\n",
        "  local_extract_path = \"../data/tmp\"\n",
        "\n",
        "# Extract the zip file\n",
        "with ZipFile(data_path) as zip_file:\n",
        "    zip_file.extractall(local_extract_path)\n",
        "\n",
        "# Collect the list of CSV file paths\n",
        "files = [os.path.join(local_extract_path, file_name) for file_name in os.listdir(local_extract_path) if file_name.endswith('.csv')]\n",
        "files.sort()\n",
        "\n",
        "# choose the file from a dropdown list inside the notebook\n",
        "df = None\n",
        "def load_file(file):\n",
        "    \"\"\"\n",
        "    Load the file into a dataframe and display the info\n",
        "    \"\"\"\n",
        "    global df\n",
        "    df = pd.read_csv(file)\n",
        "    return df.info()\n",
        "\n",
        "interact(load_file, file=files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7RW7uuApfW5"
      },
      "source": [
        "### Functions to preprocess the data\n",
        "For each file we will:\n",
        "* remove the empty columns\n",
        "* change epoch times ('frame_time_epoch') to timestamps\n",
        "* filter the rows with specific source address and keep only data frames\n",
        "* index the file by the timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0P8MhraeUFI",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# function to convert epoch times to datetime\n",
        "def convert_epoch_to_datetime(df):\n",
        "    df['timestamps'] = pd.to_datetime(df['frame_time_epoch'], unit='s')\n",
        "    return df\n",
        "\n",
        "# function to select frames with 'wlan_fc_type' == 2 and 'wlan_sa' is not 'source_address'\n",
        "def select_frames(df, source_address):\n",
        "    return df[(df['wlan_fc_type'] == 2) & (df['wlan_sa'] != source_address)]  # Is this the right way?\n",
        "\n",
        "# function to remove empty columns\n",
        "def remove_empty_columns(df):\n",
        "    return df.dropna(axis=1, how='all')\n",
        "\n",
        "def preprocess(df, source_address):\n",
        "    res = convert_epoch_to_datetime(df)\n",
        "    res = select_frames(res,source_address)\n",
        "    res = remove_empty_columns(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "# Lets apply the preprocessing function to the df we created earlier\n",
        "source_address = '00:00:00:00:00:02'  #this is the client address. we don't want packets where it is the source address\n",
        "df = preprocess(df, source_address)\n",
        "df.info()       # check the column names and their data types\n",
        "\n",
        "\n",
        "# Observations\n",
        "# - The number of columns have been reduced\n",
        "# - The 'timestamps' column is added at the end\n",
        "# - The number of rows reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah-ZX-ZicTFi",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Let us now analyze the data statistically using ydata_profiling library\n",
        "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
        "\n",
        "# Define the report path\n",
        "report_path = \"./output/report/analysis_report.html\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(report_path), exist_ok=True)\n",
        "\n",
        "# we can save the analysis in the drive\n",
        "profile.to_file(report_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXDh2c2ccTFi"
      },
      "source": [
        "### Function to resample the data\n",
        "\n",
        "The Objective is to resample the data to a specific time interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyZXJNHKcTFi",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# create a function which also takes sampling time as input\n",
        "def resample_file(df, step_size):\n",
        "\n",
        "    # if the columns exists, set index, if not, it has already been set\n",
        "    if 'timestamps' in df.columns:\n",
        "        df.set_index('timestamps', inplace=True)    #set index to timestamps\n",
        "\n",
        "    #resample data and fill missing values with previous values\n",
        "    df = df.resample(f'{step_size}s').mean(numeric_only=True).ffill()\n",
        "\n",
        "    # the first row is NaN, so we drop it\n",
        "    df = df.dropna(how = 'all')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Test the function, resample the data and plot to analyze\n",
        "step_size = 1\n",
        "resampled_df = resample_file(df, step_size)\n",
        "\n",
        "# Let's plot the 'wlan_radio_data_rate' vs timestamps\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(resampled_df.index, resampled_df['wlan_radio_data_rate'])\n",
        "plt.scatter(resampled_df.index, resampled_df['wlan_radio_data_rate'], color='red', s=10)\n",
        "plt.xlabel('timestamps')\n",
        "plt.ylabel('wlan_radio_data_rate')\n",
        "plt.title('wlan_radio_data_rate vs timestamps')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKnTG-VTcTFk"
      },
      "source": [
        "## Model Development and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWF8lSaacTFj"
      },
      "source": [
        "### Feature Creation\n",
        "The idea is to create a set of features using the past history of the data. We will use our variable of interest `wlan_radio_datarate` and its past values to create the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAZ1bdu6cTFj",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# create a function to create features from the 'wlan_radio_data_rate' column\n",
        "def create_features(df, past_history_sec):\n",
        "\n",
        "    # convert past_history from seconds to number of samples, based on the resampling time\n",
        "    past_history_samples = int(past_history_sec / step_size)\n",
        "    df = df.copy()\n",
        "    for i in range(1, past_history_samples+1):\n",
        "        df[f'data_rate_lag_{i}'] = df['wlan_radio_data_rate'].shift(i)\n",
        "\n",
        "    # remove initial rows with NaN values in the new columns\n",
        "    df = df.iloc[past_history_samples:]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Test the function, create features and plot to analyze\n",
        "past_history_sec = 15  #seconds\n",
        "\n",
        "df = create_features(resampled_df, past_history_sec)\n",
        "show(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBezAlHacTFj",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Let us now analyze the data statistically using ydata_profiling library\n",
        "profile2 = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
        "\n",
        "# Define the report path\n",
        "report_path2 = \"./output/report/analysis_report2.html\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(report_path2), exist_ok=True)\n",
        "\n",
        "# we can save the analysis in the drive\n",
        "profile.to_file(report_path2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umktCIoTcTFj",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Lets do all of the above steps on all the files in the folder in a loop and then save the results in a big dataframe\n",
        "\n",
        "df_complete = pd.DataFrame()\n",
        "\n",
        "# Select the step size and past history\n",
        "step_size = 1\n",
        "past_history = 15\n",
        "\n",
        "past_history_samples = int(past_history_sec / step_size)\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(file)\n",
        "    df = preprocess(df, source_address)\n",
        "    df = resample_file(df, step_size)\n",
        "\n",
        "    df = create_features(df, past_history)\n",
        "\n",
        "    df_complete = pd.concat([df_complete, df])\n",
        "\n",
        "show(df_complete)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfl0wgvWcTFk"
      },
      "source": [
        "### The ML Process\n",
        "<img src=\"https://github.com/b-yond-infinite-network/sharkfest-us-2024/blob/main/assets/ml_process.jpg?raw=1\">\n",
        "\n",
        "### Evaluation Metrics\n",
        "<img src=\"https://github.com/b-yond-infinite-network/sharkfest-us-2024/blob/main/assets/confusion_matrix.png?raw=1\">\n",
        "\n",
        "<img src=\"https://github.com/b-yond-infinite-network/sharkfest-us-2024/blob/main/assets/precision_recall.jpg?raw=1\">\n",
        "\n",
        "also references.\n",
        "refer to further reading.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwroqb4wcTFk",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# create a list of feature names to select from the dataframe\n",
        "features = [f'data_rate_lag_{i}' for i in range(past_history_samples, 0, -1)]\n",
        "\n",
        "features.append('wlan_radio_data_rate')\n",
        "\n",
        "target = 'label'\n",
        "\n",
        "X = df_complete[features]\n",
        "y = df_complete[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=20,\n",
        "                                                    shuffle=False\n",
        "                                                    )\n",
        "\n",
        "#  random forest classifier\n",
        "rf = RandomForestClassifier(\n",
        "                            class_weight= 'balanced',\n",
        "                            random_state=20\n",
        "                            )\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHH9nEBucTFl",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Evaluation of the model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model(y_test, y_predict):\n",
        "\n",
        "    print(\"Overall accuracy', %.3f\" %accuracy_score(y_test, y_predict))\n",
        "    precision = precision_score(y_test, y_predict)\n",
        "    print('Precision: %.3f' % precision)\n",
        "\n",
        "    recall = recall_score(y_test, y_predict)\n",
        "    print('Recall: %.3f' % recall)\n",
        "\n",
        "    f1 = f1_score(y_test, y_predict, average='binary')\n",
        "    print('F1: %.3f' % f1)\n",
        "\n",
        "    print(\"CLASSIFICATION REPORT\")\n",
        "    print(classification_report(y_test, y_predict))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "    labels = np.asarray(\n",
        "        [ [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten() ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\", cmap=\"YlGnBu\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "\n",
        "# call the function\n",
        "evaluate_model(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ViJ8A-cTFl"
      },
      "source": [
        "## Model Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5DPrcYqcTFl"
      },
      "source": [
        "We will optimize this model at two levels:\n",
        "\n",
        "First by changing the threshold that separates the positive and negative classes, then by modifying the `step_size` and `past_history` parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvGPFfsscTFl",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Find the best threshold for the model\n",
        "\n",
        "from sklearn.model_selection import FixedThresholdClassifier\n",
        "\n",
        "thresholds = np.arange(0.2, 0.9, 0.05)\n",
        "\n",
        "def find_best_threshold(classifier, X_train, y_train, X_test, y_test, thresholds):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        rf_improved = FixedThresholdClassifier(classifier, threshold=threshold)\n",
        "        y_pred = rf_improved.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "        # if any of the precision or recall is zero, skip the threshold. sklearn sets them to zero when their denominator is zero\n",
        "        if precision == 0 or recall == 0:\n",
        "            continue\n",
        "\n",
        "        results.append([threshold, precision, recall, f1])\n",
        "\n",
        "    results = pd.DataFrame(results, columns=['threshold', 'precision', 'recall', 'f1_score'])\n",
        "    results['diff_pre_rec'] = abs(results['precision'] - results['recall'])\n",
        "\n",
        "    # find the threshold with the minimum difference between precision and recall\n",
        "    best_threshold = results.loc[results['diff_pre_rec'].idxmin()]\n",
        "\n",
        "    # train the model with the best threshold and return it\n",
        "    best_model = FixedThresholdClassifier(classifier, threshold=best_threshold['threshold'])\n",
        "\n",
        "    return best_threshold, best_model\n",
        "\n",
        "# # Call the function\n",
        "# best_threshold, best_model = find_best_threshold(rf, X_train, y_train, X_test, y_test, thresholds)\n",
        "# print('Best threshold:', best_threshold['threshold'])\n",
        "# best_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate the model with the best threshold\n",
        "# y_pred = best_model.predict(X_test)\n",
        "# evaluate_model(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z1O06oNcTFl",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "## Iteratively testing various step size and past history values to optimize model performance\n",
        "%%time\n",
        "step_size_vals = [1, 2, 3, 5]\n",
        "past_history_vals = [5, 10, 15, 20, 25]\n",
        "\n",
        "# Load and preprocess the CSV files outside the inner loop\n",
        "wifi_data_list = []\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(file)\n",
        "    df = preprocess(df, source_address)\n",
        "    wifi_data_list.append(df)\n",
        "\n",
        "# Loop over the step size and history values\n",
        "final_results = []\n",
        "for step_size in step_size_vals:\n",
        "\n",
        "    for past_history in past_history_vals:\n",
        "\n",
        "        print(f'Resampling time: {step_size}, Window size: {past_history}...')\n",
        "\n",
        "        # if the window size is less than the resampling time, skip the iteration\n",
        "        if past_history < step_size:\n",
        "            continue\n",
        "\n",
        "        past_history_samples = int(past_history / step_size)\n",
        "\n",
        "        wifi_data = pd.DataFrame()\n",
        "\n",
        "        for df in wifi_data_list:\n",
        "            resampled_df = resample_file(df, step_size)\n",
        "            features_df = create_features(resampled_df, past_history)\n",
        "            wifi_data = pd.concat([wifi_data, features_df])\n",
        "\n",
        "        # create a list of features to select from the dataframe\n",
        "        features = [f'data_rate_lag_{i}' for i in range(past_history_samples, 0, -1)]\n",
        "        features.append('wlan_radio_data_rate')\n",
        "\n",
        "        X = wifi_data[features]\n",
        "        y = wifi_data[target]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                            test_size=0.2,\n",
        "                                                            random_state=10,\n",
        "                                                            shuffle=False\n",
        "                                                            )\n",
        "\n",
        "        model = RandomForestClassifier(class_weight= 'balanced',\n",
        "                                    random_state=20\n",
        "                                    )\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # finding an optimal threshold value for the model\n",
        "        best_threshold, best_model = find_best_threshold(model, X_train, y_train, X_test, y_test, thresholds)\n",
        "\n",
        "        # select the best model metrics and store them\n",
        "        precision = best_threshold['precision']\n",
        "        recall = best_threshold['recall']\n",
        "        f1 = best_threshold['f1_score']\n",
        "\n",
        "        final_results.append([step_size, past_history, precision, recall, f1, model, X_test, y_test])\n",
        "\n",
        "final_results = pd.DataFrame(final_results, columns=['step_size', 'past_history', 'precision', 'recall', 'f1_score', 'model', 'X_test', 'y_test'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnE4Zip1siZY"
      },
      "outputs": [],
      "source": [
        "# Identify the best-performing combination\n",
        "best_result = final_results.loc[final_results['f1_score'].idxmax()]\n",
        "\n",
        "# Extract the best step_size, past_history, X_test, and y_test\n",
        "step_size = best_result['step_size']\n",
        "past_history = best_result['past_history']\n",
        "X_test = best_result['X_test']\n",
        "y_test = best_result['y_test']\n",
        "\n",
        "print(f'Best Step Size: {step_size}, Best Past History: {past_history}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdCh4m6EcTFl",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Plot the results for all combinations of step size and history\n",
        "df_plot = final_results.pivot(index='step_size', columns='past_history', values='f1_score')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_plot, annot=True, fmt=\".3f\", cmap=\"BuPu\")\n",
        "plt.ylabel(\"Step size (sec)\")\n",
        "plt.xlabel(\"Past history (sec)\")\n",
        "plt.title(\"F1-score (positive class)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD9pO11ncTFm"
      },
      "source": [
        "<!-- take examples.  -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3M9Xa4dsw8y"
      },
      "source": [
        "## Application of the optimized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPbFGDYcTFm",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "positive_indices = y_test[y_test == 1].sample(10, random_state=10).index\n",
        "negative_indices = y_test[y_test == 0].sample(10, random_state=10).index\n",
        "\n",
        "# Create a figure and subplots\n",
        "fig, axs = plt.subplots(2, 1, figsize=(8, 6))\n",
        "# Plot positive_indices in red\n",
        "axs[0].plot(X_test.loc[positive_indices].T, 'r', alpha=0.5)\n",
        "axs[0].set_title('Features for label 1')\n",
        "axs[0].set_xticklabels([])\n",
        "\n",
        "# Plot negative_indices in blue\n",
        "axs[1].plot(X_test.loc[negative_indices].T, 'b', alpha=0.5)\n",
        "axs[1].set_title('Features for label 0')\n",
        "axs[1].set_xticklabels(X_test.columns, rotation=45)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIxcuheo3H67"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this workshop, we covered:\n",
        "- How to process wifi data for a prediction model including handling missing values, duplicates, and irrelevant entries\n",
        "- The importance of resampling data to ensure consistency in time intervals for time-series analysis.\n",
        "- How to create time-series features from our data.\n",
        "- The steps involved in training a machine learning model using the generated features and evaluating its performance.\n",
        "- The significance of analyzing the effect of different parameters, such as resampling time intervals and window sizes, on the model's performance and generalization ability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7hRd4363Pg8"
      },
      "source": [
        "# Future direction\n",
        "\n",
        "@Murat"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}